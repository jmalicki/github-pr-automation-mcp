# Design Philosophy: Dumb Tool, Smart Agent

## Core Principle

**This tool is philosophically opposed to interpretation.**

We are a **data access layer** and **command generator**, not an AI assistant. The AI agent consuming our tools is already intelligent - we exist to make GitHub data accessible and actionable, not to compete with the agent's intelligence.

## Responsibilities

### ‚úÖ Tool Responsibilities (Mechanical Work)

**Data Access**:

- Fetch from GitHub REST API
- Handle pagination
- Manage rate limiting
- Normalize responses

**Data Transformation**:

- Parse PR identifiers
- Format timestamps
- Combine related data (review comments + issue comments)
- Sort/filter by objective criteria

**Command Generation**:

- Generate GitHub CLI commands
- Provide action templates
- Include safety warnings

**Objective Facts Only**:

- Is this user a bot? (from GitHub API `type` field)
- What file/line? (from GitHub API)
- When created? (from GitHub API)
- How many reactions? (from GitHub API)

### ‚ùå Tool NEVER Does (Intelligent Work)

**Interpretation**:

- What does this comment mean?
- Is this important?
- Is this blocking?
- Should we fix this?

**Categorization**:

- Severity levels
- Priority scoring
- Keyword-based classification
- Sentiment analysis

**Decision Making**:

- When to resolve
- What to respond
- Which action to take
- Whether to escalate

**Content Generation**:

- Response text
- Fix suggestions
- Explanations

## Why This Boundary Matters

### 1. **Single Responsibility**

Each component does one thing well:

- Tool: GitHub data access
- Agent: Intelligence and decision-making

### 2. **Avoids Duplication**

The agent is ALREADY good at:

- Understanding natural language
- Categorizing urgency
- Making decisions
- Writing responses

We don't need to poorly replicate these capabilities.

### 3. **Flexibility**

Different agents may have different priorities:

- One agent might prioritize security comments
- Another might prioritize maintainer comments
- A third might focus on quick wins

By providing raw data, we let each agent decide.

### 4. **Maintainability**

- Interpretation logic is complex and subjective
- Keyword lists go stale
- Different projects have different priorities
- Keeping this in the agent (not the tool) keeps our code simple

## Real-World Example

**CodeRabbit Review Comment**:

```
_‚ö†Ô∏è Potential issue_ | _üü† Major_

**Avoid double /user requests...**
```

### ‚ùå Bad Approach (Tool interprets)

```typescript
// Tool tries to be smart
{
  body: "‚ö†Ô∏è Potential issue...",
  severity: "high",          // Tool parsed emoji
  category: "performance",   // Tool guessed
  is_blocking: false,        // Tool decided
  suggested_response: "..."  // Tool wrote content
}
```

**Problems**:

- What if agent disagrees with "high" severity?
- What if this project doesn't care about performance?
- What if the suggested response is wrong?
- Now we have TWO AIs making conflicting decisions

### ‚úÖ Good Approach (Tool provides, Agent decides)

```typescript
// Tool provides raw data + commands
{
  body: "_‚ö†Ô∏è Potential issue_ | _üü† Major_\n\n**Avoid double /user requests...**",
  action_commands: {
    reply_command: "gh pr comment 2 --body 'YOUR_RESPONSE_HERE'",
    resolve_command: "gh api -X POST .../replies -f body='‚úÖ Fixed'",
    resolve_condition: "Run ONLY after verifying fix for: 'Avoid double /user requests...'"
  }
}
```

**Agent's workflow**:

1. Reads body
2. Sees "_üü† Major_" marker
3. Parses "Avoid double /user requests"
4. Decides: "This is valid, I'll fix it"
5. Makes the fix
6. Writes response: "Good catch! Fixed in commit abc123..."
7. Executes: `gh pr comment 2 --body "Good catch! Fixed in commit abc123..."`
8. Verifies fix is in PR
9. Executes resolve command

**Benefits**:

- Agent makes all intelligent decisions
- Tool just provides data and commands
- Agent can parse CodeRabbit format (agent is good at this!)
- No duplication of intelligence

## Guidelines for Implementation

### When adding new tool features, ask

**"Is this a fact or an opinion?"**

- Fact ‚Üí Tool can provide it
- Opinion ‚Üí Agent decides

**"Can GitHub API tell us this objectively?"**

- Yes ‚Üí Tool returns it
- No ‚Üí Agent interprets it

**"Does this require understanding natural language?"**

- No ‚Üí Tool can do it (e.g., sort by file name)
- Yes ‚Üí Agent does it (e.g., "is this urgent?")

**"Would different agents want different answers?"**

- Yes ‚Üí Agent decides
- No ‚Üí Tool can standardize

## Examples

| Feature | Tool or Agent? | Why |
|---------|---------------|-----|
| Fetch comment body | ‚úÖ Tool | Objective API call |
| Sort by creation date | ‚úÖ Tool | Objective comparison |
| Filter bots | ‚úÖ Tool | GitHub API provides `type: "Bot"` |
| Parse PR identifier | ‚úÖ Tool | Deterministic regex |
| Generate reply command | ‚úÖ Tool | Mechanical template |
| **Decide if comment is important** | ‚ùå Agent | Subjective, context-dependent |
| **Categorize severity** | ‚ùå Agent | Opinion, varies by project |
| **Write response text** | ‚ùå Agent | Creative, context-dependent |
| **Decide when to resolve** | ‚ùå Agent | Requires verification |
| **Parse CodeRabbit severity** | ‚ùå Agent | Agent already good at NLP |

## Summary

**We're plumbing, not intelligence.**

Our value:

- ‚úÖ Handle GitHub API complexity
- ‚úÖ Provide commands agent can execute
- ‚úÖ Make review workflow scriptable

Not our value:

- ‚ùå Duplicate agent's NLP abilities
- ‚ùå Make decisions agent should make
- ‚ùå Add interpretation layer

**The agent is smart. We help it act on GitHub.**
